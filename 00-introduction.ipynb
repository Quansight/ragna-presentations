{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ragna-logo.png\" width=30% align=\"right\"/>\n",
    "\n",
    "# Build an AI Document Inquiry Chat with Offline LLMs\n",
    "\n",
    "**Tutorial at PyCon US 2024**\n",
    "\n",
    "**Room: 321**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ragna-web-ui.gif\" width=80% />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenters üë©üèª‚Äçüè´\n",
    "\n",
    "<img src=\"images/quansight.png\" width=35% align=\"right\"/>\n",
    "\n",
    "* Pavithra Eswaramoorthy, [@pavithraes](https://github.com/pavithraes), Developer Advocate at Quansight\n",
    "* Dharhas Pothina, [@dharahs](https://github.com/dharahs), CTO at Quansight\n",
    "* Andrew Huang,[@ahuang11](https://github.com/ahuang11), Software Engineer at Anaconda\n",
    "\n",
    "## Setup üßë‚Äçüíª\n",
    "\n",
    "1. Go to `pycon-tutorial.quansight.dev`\n",
    "2. Sign-in with a `<username>` and `<password>` provided\n",
    "3. Click on the JupyterLab card, and select and start the GPU server\n",
    "4. In the left sidebar, navigate to `tutorial/00-introduction.ipynb` and follow along!\n",
    "\n",
    "‚òùüèº **Note:** Ensure you are using the `global-global-pycon` for all notebooks. This environment should be auto-selected for you.\n",
    "\n",
    "That's it, you can now follow along with the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: What is Nebari - the platform we're using? ‚ÑπÔ∏è\n",
    "\n",
    "It's an open source data science platform. It is quick to deploy (by anyone) and ships with several useful features like environment management and dashboard deployment.\n",
    "\n",
    "üîó Learn more: [nebari.dev](https://nebari.dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's goals üéØ\n",
    "\n",
    "* Run a Large Language Model (LLM) in offline mode\n",
    "* Build a RAG-based chat function with Ragna's Python API, OpenAI's GPT 4, and a local LLM\n",
    "* Build and deploy a web application with Panel to interact with the LLM\n",
    "* Explore a Panel-based web app built with Ragna, and experiment with different options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**‚ú® Next: [Set up an offline Large Language Model](01-local-llm.ipynb) ‚Üí**\n",
    "\n",
    "üí¨ _Wish to continue discussions after the tutorial? Contact the presenters: [@pavithraes](https://github.com/pavithraes), [@dharahs](https://github.com/dharahs), [@ahuang11](https://github.com/ahuang11)_\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peswaramoorthy@quansight.com-peswaramoorthy@quansight.com-pycon-de",
   "language": "python",
   "name": "conda-env-peswaramoorthy_quansight.com-peswaramoorthy_quansight.com-pycon-de-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
