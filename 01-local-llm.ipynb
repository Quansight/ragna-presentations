{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4068322e-24d0-49db-a092-1ef9fadbf7df",
   "metadata": {},
   "source": [
    "<img src=\"images/ragna-logo.png\" width=15% align=\"right\"/>\n",
    "\n",
    "# Set up an offline Large Language Model\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93e56c60-3aa0-42cf-8b8e-07e94f29dc5f",
   "metadata": {},
   "source": [
    "## What is a Large Language Model (LLM)?\n",
    "\n",
    "A \"language model\" is a machine learning model designed to understand and generate (predict) natural language. For example, auto-completion of text in input fields often use language models.\n",
    "\n",
    "A \"large language model\" is a language model based on the [Transformer architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)), trained on large amounts of (general) data and consists of several million to billion parameters. With this scale and complexity, LLMs are capable of various text processing and generation tasks like document summarization, answering common questions, text-based content creation.\n",
    "\n",
    "Popular LLMs include Open AI's GPT, Google's Gemini, Anthropic's Claude, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023605d-8321-4854-8ce0-462847b479dc",
   "metadata": {},
   "source": [
    "## What is a \"local\" or \"offline\" LLM\n",
    "\n",
    "Large Language Models (LLMs) like OpenAI's GPT are proprietary, can only be accessed through the OpenAI API or services like ChatGPT. While easy to use, these can be concerning for data privacy, vendor lock-in, and cost-related reasons.\n",
    "\n",
    "Offline, local, or open weight LLMs are models that can be self-hosted on your local computers.\n",
    "\n",
    "Today, we're running it on a cloud platform, but each of you have access to essentially an individual machine. This allows us to have a standard tutorial environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb3e8-c0df-4cac-bc12-f36a87a7ba9e",
   "metadata": {},
   "source": [
    "## LLM: Mistral 7B\n",
    "\n",
    "In this tutorial, we'll use the Mistral 7B model, which is released under the Apache 2.0 license.\n",
    "\n",
    "This is a well performing and popular model for offline use. Learn more at [mistral.ai](https://mistral.ai/news/announcing-mistral-7b/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbc7e7-24a8-40a7-9fcf-60bcfda88451",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "> Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
    "> \n",
    "> Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n",
    "> \n",
    "> ~ [Hugging Face Documentation](https://huggingface.co/docs/optimum/en/concept_guides/quantization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c61f-65a8-4461-866c-4a609765a563",
   "metadata": {},
   "source": [
    "## Exllamav2\n",
    "\n",
    "A quantization and inference library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ce6bf-8e4b-4cb5-8662-b1d835320a10",
   "metadata": {},
   "source": [
    "To download this locally,\n",
    "\n",
    "1. Install `Exllamav2`:\n",
    "2. In the terminal: `git lfs install` and `git clone https://huggingface.co/turboderp/Mistral-7B-v0.2-exl2`\n",
    "3. View all branches:`git branch --all`\n",
    "4. Check-out the weights of your choice `git checkout remotes/origin/2.5bpw`\n",
    "5. Note the model directory path, and use it in the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3e81b-78bc-4919-8e5b-80379099de67",
   "metadata": {},
   "source": [
    "Let's run the example inference script: https://github.com/turboderp/exllamav2/blob/master/examples/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da86298-2c2d-4b8d-bcde-48d2d0f582ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:40:23.892345Z",
     "iopub.status.busy": "2024-04-18T07:40:23.892059Z",
     "iopub.status.idle": "2024-04-18T07:40:23.896647Z",
     "shell.execute_reply": "2024-04-18T07:40:23.896035Z",
     "shell.execute_reply.started": "2024-04-18T07:40:23.892324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf325fc8-d9c5-47d2-b4e6-3236d619519f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:40:00.524915Z",
     "iopub.status.busy": "2024-04-18T07:40:00.524324Z",
     "iopub.status.idle": "2024-04-18T07:40:00.528373Z",
     "shell.execute_reply": "2024-04-18T07:40:00.527638Z",
     "shell.execute_reply.started": "2024-04-18T07:40:00.524887Z"
    }
   },
   "outputs": [],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a49e16-6634-420a-9012-1af1cffafa3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:40:01.104036Z",
     "iopub.status.busy": "2024-04-18T07:40:01.103749Z",
     "iopub.status.idle": "2024-04-18T07:40:01.108695Z",
     "shell.execute_reply": "2024-04-18T07:40:01.107787Z",
     "shell.execute_reply.started": "2024-04-18T07:40:01.104014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/peswaramoorthy@quansight.com/shared/developer/pavithraes/Mistral-7B-v0.2-exl2\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and cache\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "dir_relative_path = \"shared/developer/pavithraes/Mistral-7B-v0.2-exl2/\"\n",
    "\n",
    "model_directory =  str(Path.home() / dir_relative_path)\n",
    "print(\"Loading model: \" + model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4dfa40b-1a98-41f0-945b-a7369b19ca8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:40:42.629327Z",
     "iopub.status.busy": "2024-04-18T07:40:42.628793Z",
     "iopub.status.idle": "2024-04-18T07:40:42.818114Z",
     "shell.execute_reply": "2024-04-18T07:40:42.817526Z",
     "shell.execute_reply.started": "2024-04-18T07:40:42.629302Z"
    }
   },
   "outputs": [],
   "source": [
    "config = ExLlamaV2Config()\n",
    "\n",
    "config.model_dir = model_directory\n",
    "\n",
    "config.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a6c812a-8767-4617-884a-05707d0bc251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:41:08.872571Z",
     "iopub.status.busy": "2024-04-18T07:41:08.872275Z",
     "iopub.status.idle": "2024-04-18T07:41:08.876668Z",
     "shell.execute_reply": "2024-04-18T07:41:08.875921Z",
     "shell.execute_reply.started": "2024-04-18T07:41:08.872551Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ExLlamaV2(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75992b8d-c20c-4d6f-a89c-5368c92552a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:41:10.561304Z",
     "iopub.status.busy": "2024-04-18T07:41:10.560539Z",
     "iopub.status.idle": "2024-04-18T07:41:10.564584Z",
     "shell.execute_reply": "2024-04-18T07:41:10.563751Z",
     "shell.execute_reply.started": "2024-04-18T07:41:10.561276Z"
    }
   },
   "outputs": [],
   "source": [
    "cache = ExLlamaV2Cache(model, lazy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93f6d00-1ecf-476b-989e-11643c0aba2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:41:11.675125Z",
     "iopub.status.busy": "2024-04-18T07:41:11.674843Z",
     "iopub.status.idle": "2024-04-18T07:43:04.176081Z",
     "shell.execute_reply": "2024-04-18T07:43:04.175432Z",
     "shell.execute_reply.started": "2024-04-18T07:41:11.675104Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_autosplit(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c825e95-782d-43e9-8b9c-638c19bd36f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:45:40.016770Z",
     "iopub.status.busy": "2024-04-18T07:45:40.015983Z",
     "iopub.status.idle": "2024-04-18T07:45:41.206747Z",
     "shell.execute_reply": "2024-04-18T07:45:41.206043Z",
     "shell.execute_reply.started": "2024-04-18T07:45:40.016740Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = ExLlamaV2Tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7659d4f8-6b00-4305-a727-de47785d70a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:45:49.295810Z",
     "iopub.status.busy": "2024-04-18T07:45:49.295514Z",
     "iopub.status.idle": "2024-04-18T07:45:49.299071Z",
     "shell.execute_reply": "2024-04-18T07:45:49.298518Z",
     "shell.execute_reply.started": "2024-04-18T07:45:49.295789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "348864af-bf6c-4e9c-abb3-df70b5e80bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T07:45:49.666707Z",
     "iopub.status.busy": "2024-04-18T07:45:49.665798Z",
     "iopub.status.idle": "2024-04-18T07:46:00.302384Z",
     "shell.execute_reply": "2024-04-18T07:46:00.301697Z",
     "shell.execute_reply.started": "2024-04-18T07:45:49.666680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our story begins in the Scottish town of Auchtermuchty, where once, long ago, in the early 1900s, there was a small shop called C.S. Robertson & Co. The shop sold a lot of things, but the most important thing it sold was coal. And in a town like Auchtermuchty, with its harsh winters and its poor housing, coal was essential. It warmed the homes of the townspeople. It heated the factories. It kept the wheels of industry turning.\n",
      "\n",
      "But coal was not cheap. And coal was not easy to come by. In those days, Auchtermuchty was a poor town. Many of its people lived in cramped, damp, unheated houses. Many of its people had trouble making ends\n",
      "\n",
      "Response generated in 8.69 seconds, 150 tokens, 17.26 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.01\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
    "\n",
    "prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
    "\n",
    "max_new_tokens = 150\n",
    "\n",
    "generator.warmup()\n",
    "time_begin = time.time()\n",
    "\n",
    "output = generator.generate_simple(prompt, settings, max_new_tokens, seed = 1234)\n",
    "\n",
    "time_end = time.time()\n",
    "time_total = time_end - time_begin\n",
    "\n",
    "print(output)\n",
    "print()\n",
    "print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e88ab-3934-44d4-87a2-b01c32457af0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**✨ Next: [Basics of RAG-powered chat app](02-rag-basics.ipynb) →**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06613cd-876d-4804-9da5-6bfaac912ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peswaramoorthy@quansight.com-peswaramoorthy@quansight.com-pycon-de",
   "language": "python",
   "name": "conda-env-peswaramoorthy_quansight.com-peswaramoorthy_quansight.com-pycon-de-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
