{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c14620e-982a-46bd-b621-6f2cb60d0a13",
   "metadata": {},
   "source": [
    "<img src=\"images/ragna-logo.png\" width=\"200px\" align=\"right\"/>\n",
    "\n",
    "# RAG and LLM Experiments\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e205f5-3201-4e55-8350-397d5e4746ba",
   "metadata": {},
   "source": [
    "## Explore the Web UI\n",
    "\n",
    "Ragna ships with a Panel-based chat application, sometimes also referred to as the web UI. You can use this directly, or as an example to build your own applications.\n",
    "\n",
    "Before you can run the Ragna UI, you need to create a config `ragna.toml` file. This can be done with an interactive wizard by running `ragna init` in a terminal. \n",
    "\n",
    "In a cloud environment like Nebari there are a few extra configurations items needed. For this tutorial you can create the config file by running the next cell.\n",
    "\n",
    "This will create write `ragna.toml` file to your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875ce6c-c84b-429f-8537-b1700e53a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ragna config file for Nebari and place it at ~/ragna.toml\n",
    "\n",
    "import os\n",
    "with open(f\"{os.environ['HOME']}/ragna.toml\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "# Replace the $USER occurrences in this file\n",
    "# with the username that you logged in\n",
    "\n",
    "local_root = \"./.cache/ragna\"\n",
    "authentication = \"ragna.deploy.InJupyterHubAuthentication\"\n",
    "document = \"ragna.core.LocalDocument\"\n",
    "source_storages = [\n",
    "    \"ragna.source_storages.RagnaDemoSourceStorage\",\n",
    "    \"ragna.source_storages.Chroma\",\n",
    "    \"ragna.source_storages.LanceDB\",\n",
    "]\n",
    "assistants = [\n",
    "    \"ragna.assistants.RagnaDemoAssistant\",\n",
    "    \"ragna.assistants.Gpt35Turbo16k\",\n",
    "    \"ragna.assistants.Gpt4\",\n",
    "    \"local_llm.Llama38BInstruct\",\n",
    "]\n",
    "\n",
    "[api]\n",
    "hostname = \"127.0.0.1\"\n",
    "port = 31476\n",
    "root_path = \"/user/{os.environ['JUPYTERHUB_USER']}/proxy/31476/\"\n",
    "url = \"https://pycon-tutorial.quansight.dev/user/{os.environ['JUPYTERHUB_USER']}/proxy/31476/\"\n",
    "database_url = \"sqlite:///./.cache/ragna/ragna.db\"\n",
    "origins = [\n",
    "    \"https://pycon-tutorial.quansight.dev\",\n",
    "]\n",
    "\n",
    "[ui]\n",
    "hostname = \"127.0.0.1\"\n",
    "port = 31477\n",
    "origins = [\n",
    "    \"https://pycon-tutorial.quansight.dev\",\n",
    "]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf5a5d-1856-432a-8241-e8469d18c940",
   "metadata": {},
   "source": [
    "### To run the Ragna UI from Nebari, open a terminal window and run the following commands. \n",
    "\n",
    "1. Activate the conda environment\n",
    "   \n",
    "```bash\n",
    "conda activate pycon-ragna\n",
    "```\n",
    "\n",
    "2. Start the UI (this may take a few minutes to launch)\n",
    "\n",
    "```bash\n",
    "dotenv --file ~/shared/pycon/.env run -- python -m ragna ui --config ~/ragna.toml\n",
    "```\n",
    "\n",
    "3. Run the cell below to generate the link to visit the Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5cc1a-205c-42ea-9b18-60014aa0dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web UI Link\n",
    "import os \n",
    "print(f\"https://pycon-tutorial.quansight.dev/user/{os.environ['JUPYTERHUB_USER']}/proxy/31477/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaefe2-cfae-40e9-b573-71b6a68088e2",
   "metadata": {},
   "source": [
    "### Side note: Local setup instructions 💻\n",
    "\n",
    "On your personal computers, you can directly run: `ragna ui` to start the UI and go to `http://localhost:31477` to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87167db3-9f9c-43fa-a3a9-cf4801e7519a",
   "metadata": {},
   "source": [
    "## Compare LLMs\n",
    "\n",
    "Orchestration tools like Ragna can be useful for comparing and experimenting with LLMs quickly. \n",
    "\n",
    "In the following cells, let's see how our local LLM, Llama3-8B, compares to OpenAI's GPT 3.5 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2d246-e5f1-49e0-a8b0-acf97094ebd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from ragna import Rag\n",
    "from ragna.assistants import Gpt4, Gpt35Turbo16k\n",
    "from ragna.source_storages import Chroma\n",
    "\n",
    "from local_llm import Llama38BInstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47daac6-eb93-425c-9999-5c4d9d19d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path.home() / Path(\"shared/pycon/.env\")\n",
    "assert load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a2a56-d10f-4495-8ffb-be528d392051",
   "metadata": {},
   "source": [
    "Let's inquire about PSF's annual reports again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5e104-82d3-49d1-ace5-080649f9c058",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"files/psf-report-2021.pdf\",\n",
    "    \"files/psf-report-2022.pdf\",\n",
    "    \"files/psf-report-2023.pdf\",\n",
    "]\n",
    "\n",
    "source_storages = [Chroma]\n",
    "assistants = [Gpt35Turbo16k, Gpt4, Llama38BInstruct]\n",
    "\n",
    "prompt = \"What was PSF's net income in 2021, 2022, and 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3608b-8647-413b-9542-be50572a583c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = Rag()\n",
    "\n",
    "async def answer_prompt(source_storage, assistant):\n",
    "    async with rag.chat(\n",
    "        documents=documents,\n",
    "        source_storage=source_storage,\n",
    "        assistant=assistant,\n",
    "    ) as chat:\n",
    "        message = await chat.answer(prompt)\n",
    "        return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02054a04-27f3-493a-8f3f-82207adc31ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    (source_storage.display_name(), assistant.display_name()): answer_prompt(\n",
    "        source_storage, assistant\n",
    "    )\n",
    "    for source_storage, assistant in itertools.product(source_storages, assistants)\n",
    "}\n",
    "\n",
    "pprint(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cacba-7c6e-4d88-9f2b-41e7213c0879",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = dict(zip(experiments.keys(), await asyncio.gather(*experiments.values())))\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4570d0-a653-43ec-91a1-7db4645ab661",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "_❗️ **Warning:** Make sure to stop the Jupyter Kernel (in the JupyterLab Menu Bar, click on \"Kernel\" -> \"Shut down Kernel\") before proceeding to prevent the \"insufficient VRAM\" error._\n",
    "\n",
    "<br>\n",
    "\n",
    "**✨ Next: [Conclusion](07-conclusion.ipynb) →**\n",
    "\n",
    "💬 _Wish to continue discussions after the tutorial? Contact the presenters: [@pavithraes](https://github.com/pavithraes), [@dharhas](https://github.com/dharhas), [@ahuang11](https://github.com/ahuang11)_\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon-pycon-ragna",
   "language": "python",
   "name": "conda-env-pycon-pycon-ragna-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
