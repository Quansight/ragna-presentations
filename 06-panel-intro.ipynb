{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to HoloViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoloViz is a suite of high-level Python tools that are designed to work together to make visualizing data a breeze, from conducting exploratory data analysis to deploying complex dashboards.\n",
    "\n",
    "The core HoloViz projects are as follow,\n",
    "\n",
    "- [Panel](https://panel.holoviz.org): Create interactive dashboards in Jupyter notebooks or standalone apps\n",
    "- [hvPlot](https://hvplot.holoviz.org): Quickly and interactively explore data with a familiar API\n",
    "- [HoloViews](https://holoviews.org): Interactive plotting experience\n",
    "- [GeoViews](http://geoviews.org): Geographic extension of HoloViews\n",
    "- [Datashader](https://datashader.org): Render big data images in a browser\n",
    "- [Lumen](https://lumen.holoviz.org/): Construct no-code dashboards from simple YAML specifications\n",
    "- [Colorcet](https://colorcet.holoviz.org/): Plot with perceptually based colormaps\n",
    "- [Param](https://param.holoviz.org): Declaratively code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Panel\n",
    "\n",
    "Today, the focus is on Panel.\n",
    "\n",
    "Panel packs many pre-built frontend components that are **usable with Python**.\n",
    "\n",
    "That means you can convert your static Python scripts into interactive ones--**no Javascript necessary**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Panel Tutorial\n",
    "\n",
    "Let's start out building an interactive app that allows the user to print a custom message.\n",
    "\n",
    "Currently it's hard coded to `\"Hello World\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give the user more control by introducing a `TextInput` widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = pn.widgets.TextInput(value=\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can `pn.bind` the widget's `param.value` to the callback, `echo_message`, which simply echos the input value on change.\n",
    "\n",
    "Note: it's important to prefix `value` with `param`--without it, there will be no updates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_message(message):\n",
    "    return f\"<i>{message}</i>\"\n",
    "\n",
    "message_ref = pn.bind(echo_message, message=message_input.param.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a simple layout to see the results.\n",
    "\n",
    "Try typing unique in the widget to see the message update!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(message_input, message_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we:\n",
    "\n",
    "1. instantiated a widget (`TextInput`).\n",
    "2. defined a function `echo_message`\n",
    "3. bounded the function to the widget's *param* value\n",
    "4. laid out the the widget and the bound reference\n",
    "\n",
    "![recap](images/.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's all the code cells collected into one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "message_input = pn.widgets.TextInput(value=\"Hello World!\")\n",
    "\n",
    "def echo_message(message):\n",
    "    return f\"<i>{message}</i>\"\n",
    "\n",
    "message_ref = pn.bind(echo_message, message=message_input.param.value)\n",
    "\n",
    "pn.Column(message_input, message_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this repeatedly is key to creating more complex apps with Panel, so let's do a quick exercise.\n",
    "\n",
    "Your goal is to create a widget that will toggle the message to upper case if activated by filling out the ellipses (`...`)!\n",
    "\n",
    "Hint: check out the [Component gallery](https://panel.holoviz.org/reference/index.html) to see what widgets are available to accomplish this goal (one of them starts with a T, but there are multiple solutions!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "message_input = pn.widgets.TextInput(value=\"Hello World!\")\n",
    "toggle_upper = ...\n",
    "\n",
    "def echo_message(message, toggle_upper):\n",
    "    ...\n",
    "    return f\"<i>{message}</i>\"\n",
    "\n",
    "message_ref = pn.bind(echo_message, message=message_input.param.value, toggle_upper=...)\n",
    "\n",
    "pn.Column(message_input, message_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on building an interactive Panel app! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Panel ChatInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, introducing `pn.chat.ChatInterface`, which is a component that packages all the steps you just learned to provide convenient features for developing a Chat UI with LLMs!\n",
    "\n",
    "Try typing a message and pressing enter to send!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = pn.chat.ChatInterface()\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that it echoes the message you entered, but it doesn't reply... not fun (yet).\n",
    "\n",
    "To make it reply, all we have to do is set a `callback`, like `pn.bind`, but with a caveat: it needs these three arguments: `contents`, `user`, and `instance`.\n",
    "\n",
    "Now when you try sending a message in the chat interface, it will be echoed back in italics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_message(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    return f\"<i>{contents}</i>\"\n",
    "\n",
    "chat.callback = echo_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have seen services, like OpenAI and Mistral, stream tokens as they arrive.\n",
    "\n",
    "We can simulate streaming tokens by looping through the contents of the user's input, concatenating the characters to the final message, and `yield`ing it.\n",
    "\n",
    "Since there's no serious computation, it'll run too fast for us to perceive streaming--thus `time.sleep`.\n",
    "\n",
    "Here's the latest code collected into one (and also `callback` within instantation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "def stream_echo_message(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    message = \"\"\n",
    "    for char in contents:\n",
    "        time.sleep(0.1)  # to simulate a serious computation\n",
    "        message += char\n",
    "        yield f\"<i>{message}</i>\"\n",
    "\n",
    "chat = pn.chat.ChatInterface(callback=stream_echo_message)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now let's make it much more interesting by connecting an LLM, like the quantized Mistral Instruct 7B model through llama-cpp-python (so no API key necessary)!\n",
    "\n",
    "Here, we:\n",
    "1. download the quantized model (if it doesn't exist already)\n",
    "2. instantiate the model with `Llama`\n",
    "3. serialize all messages into `transformers` format\n",
    "4. calls the chat completion Openai-like API on the messages\n",
    "5. stream the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import panel as pn\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "pn.extension()\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    ")  # 1.\n",
    "\n",
    "llama = llama_cpp.Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_ctx=2048,\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ") # 2.\n",
    "\n",
    "def stream_response(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    messages = instance.serialize()  # 3.\n",
    "    response = llama.create_chat_completion_openai_v1(messages=messages, stream=True)  # 4.\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        part = chunk.choices[0].delta.content or \"\"\n",
    "        message += part\n",
    "        yield message  # 5.\n",
    "\n",
    "chat = pn.chat.ChatInterface(callback=stream_response)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even give the model a personality by setting a system message!\n",
    "\n",
    "Note, Mistral Instruct does NOT support the `system` role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import panel as pn\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "system_message = \"You are an excessively passionate Pythonista.\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    ")  # 1.\n",
    "\n",
    "llama = llama_cpp.Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_ctx=2048,\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")  # 2.\n",
    "\n",
    "\n",
    "def stream_response(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": system_message}\n",
    "    ] + instance.serialize()  # 3.\n",
    "    response = llama.create_chat_completion_openai_v1(\n",
    "        messages=messages, stream=True\n",
    "    )  # 4.\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        part = chunk.choices[0].delta.content or \"\"\n",
    "        message += part\n",
    "        yield message  # 5.\n",
    "\n",
    "\n",
    "chat = pn.chat.ChatInterface(callback=stream_response)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this Chat UI improved by using templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pn.template.FastListTemplate(main=[chat], title=\"Chatbot\", accent=\"#A01346\")\n",
    "template.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn! Try aggregating all you've learned to customize the personality of the chatbot on the go!\n",
    "\n",
    "Again, replace the ellipses with the appropriate code snippets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import panel as pn\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "system_message = \"You are an excessively passionate Pythonista.\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    ")  # 1.\n",
    "\n",
    "llama = llama_cpp.Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_ctx=2048,\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")  # 2.\n",
    "\n",
    "\n",
    "def stream_response(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": ...}\n",
    "    ] + instance.serialize()  # 3.\n",
    "    response = llama.create_chat_completion_openai_v1(\n",
    "        messages=messages, stream=True\n",
    "    )  # 4.\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        part = chunk.choices[0].delta.content or \"\"\n",
    "        message += part\n",
    "        yield message  # 5.\n",
    "\n",
    "\n",
    "system_input = ...\n",
    "chat = pn.chat.ChatInterface(callback=stream_response)\n",
    "template = pn.template.FastListTemplate(\n",
    "    main=[chat], sidebar=[...], title=\"Chatbot\", accent=\"#A01346\"\n",
    ")\n",
    "template.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holonote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
