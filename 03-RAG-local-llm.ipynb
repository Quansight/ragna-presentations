{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6d44f7-b10f-4d06-92e2-e889445eed75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"images/ragna-logo.png\" width=25% align=\"right\"/>\n",
    "\n",
    "# Use Local LLM with Ragna\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3984d25a-6feb-4e7c-b966-a5635b8f943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "from ragna.core import Assistant, PackageRequirement, Source\n",
    "\n",
    "\n",
    "class Mistral7BInstruct(Assistant):\n",
    "    @classmethod\n",
    "    def display_name(cls):\n",
    "        return \"Mistral-7B-v0.2-exl2\"\n",
    "\n",
    "    @classmethod\n",
    "    def requirements(cls):\n",
    "        return [\n",
    "            PackageRequirement(\"exllamav2\"),\n",
    "            PackageRequirement(\"torch\"),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def is_available(cls):\n",
    "        requirements_available = super().is_available()\n",
    "        if not requirements_available:\n",
    "            return False\n",
    "\n",
    "        import torch\n",
    "\n",
    "        return torch.cuda.is_available()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from exllamav2 import (\n",
    "            ExLlamaV2,\n",
    "            ExLlamaV2Cache,\n",
    "            ExLlamaV2Config,\n",
    "            ExLlamaV2Tokenizer,\n",
    "        )\n",
    "        from exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(Path.cwd() / \"models\" / self.display_name())\n",
    "        config.prepare()\n",
    "\n",
    "        self.tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "        model = ExLlamaV2(config)\n",
    "        cache = ExLlamaV2Cache(model, lazy=True)\n",
    "        model.load_autosplit(cache)\n",
    "        self.generator = ExLlamaV2StreamingGenerator(model, cache, self.tokenizer)\n",
    "        self.generator.set_stop_conditions({self.tokenizer.eos_token})\n",
    "\n",
    "        self.settings = ExLlamaV2Sampler.Settings()\n",
    "        self.settings.temperature = 0.0\n",
    "\n",
    "    def make_prompt(self, prompt: str, sources: list[Source]) -> str:\n",
    "        return \"\".join(\n",
    "            [\n",
    "                f\"<s>[INST] \",\n",
    "                f\"You are a helpful assistant that answers prompts by only using the documents listed below. \",\n",
    "                f\"Each individual document is started pattern <doc> and ended by </doc>. \",\n",
    "                f\"If you can't answer a question based on the sources you are given, just say so. Do not make up information.\",\n",
    "                *[f\"<doc> {source.content} </doc>\" for source in sources],\n",
    "                f\"Reply with OK if you have understood these instructions.\",\n",
    "                f\" [/INST]OK</s>[INST] {prompt} [/INST]\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def answer(\n",
    "        self, prompt: str, sources: list[Source], *, max_new_tokens: int = 256\n",
    "    ) -> Iterator[str]:\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            self.make_prompt(prompt, sources), add_bos=False\n",
    "        )\n",
    "\n",
    "        self.generator.begin_stream_ex(input_ids, self.settings)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            result = self.generator.stream_ex()\n",
    "            if result[\"eos\"]:\n",
    "                break\n",
    "            yield result[\"chunk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fbad4-4fc4-4d09-9caa-5842b1cfe466",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Mistral7BInstruct.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f66c85-2b75-4fff-aced-f5cc4da40091",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = Mistral7BInstruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65704500-ad6a-44b8-9499-772d5c822de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragna import Rag, source_storages\n",
    "\n",
    "documents = [\n",
    "    \"files/what-is-ragna.txt\",\n",
    "    \"files/psf-report-2022.pdf\",\n",
    "    \"files/numfocus-report-2023.pdf\",\n",
    "]\n",
    "\n",
    "chat = Rag().chat(\n",
    "    documents=documents,\n",
    "    source_storage=source_storages.Chroma,\n",
    "    assistant=assistant,\n",
    ")\n",
    "\n",
    "await chat.prepare()\n",
    "\n",
    "message = await chat.answer(\"How many projects are sponsored by NumFOCUS?\", stream=True)\n",
    "\n",
    "for source in message.sources:\n",
    "    print(source.content)\n",
    "\n",
    "print(\"#\" * 80)\n",
    "\n",
    "async for chunk in message:\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a2e43-6d6a-46b9-a87d-9b00901d27f9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**✨ Next: TODO →**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007efd5a-7df4-4cbe-b7d6-8ab6fb1216cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
