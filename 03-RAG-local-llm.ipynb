{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6d44f7-b10f-4d06-92e2-e889445eed75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"images/ragna-logo.png\" width=\"200px\" align=\"right\"/>\n",
    "\n",
    "# Use Local LLM with Ragna\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1610ef-6fa7-438e-a76c-c89cfb8b59ec",
   "metadata": {},
   "source": [
    "## Create a new Ragna assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2c8b6-ab62-472f-98ce-24bf63a7fc45",
   "metadata": {},
   "source": [
    "@pmeier TODO - Add explanations for `make_prompt`, `answer` functions\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> <b>Expand to read <code>local_llm.py</code> üëáüèº </b></summary>\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "from ragna.core import Assistant, PackageRequirement, Source\n",
    "\n",
    "\n",
    "class Mistral7BInstruct(Assistant):\n",
    "    @classmethod\n",
    "    def display_name(cls):\n",
    "        return \"turboderp/Mistral-7B-v0.2-exl2\"\n",
    "\n",
    "    @classmethod\n",
    "    def requirements(cls):\n",
    "        return [\n",
    "            PackageRequirement(\"torch\"),\n",
    "            PackageRequirement(\"exllamav2\"),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def is_available(cls):\n",
    "        requirements_available = super().is_available()\n",
    "        if not requirements_available:\n",
    "            return False\n",
    "\n",
    "        import torch\n",
    "\n",
    "        return torch.cuda.is_available()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from exllamav2 import (\n",
    "            ExLlamaV2,\n",
    "            ExLlamaV2Cache,\n",
    "            ExLlamaV2Config,\n",
    "            ExLlamaV2Tokenizer,\n",
    "        )\n",
    "        from exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(Path.home() / \"shared/analyst/models\" / self.display_name())\n",
    "        config.prepare()\n",
    "\n",
    "        self.tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "        model = ExLlamaV2(config)\n",
    "        cache = ExLlamaV2Cache(model, lazy=True)\n",
    "        model.load_autosplit(cache)\n",
    "        self.generator = ExLlamaV2StreamingGenerator(model, cache, self.tokenizer)\n",
    "        self.generator.set_stop_conditions({self.tokenizer.eos_token})\n",
    "\n",
    "        self.settings = ExLlamaV2Sampler.Settings()\n",
    "        self.settings.temperature = 0.0\n",
    "\n",
    "    def make_prompt(self, prompt: str, sources: list[Source]) -> str:\n",
    "        return \"\".join(\n",
    "            [\n",
    "                f\"<s>[INST] \",\n",
    "                f\"You are a helpful assistant that answers prompts by only using the documents listed below. \",\n",
    "                f\"Each individual document is started pattern <doc> and ended by </doc>. \",\n",
    "                f\"If you can't answer a question based on the sources you are given, just say so. Do not make up information.\",\n",
    "                *[f\"<doc> {source.content} </doc>\" for source in sources],\n",
    "                f\"Reply with OK if you have understood these instructions.\",\n",
    "                f\" [/INST]OK</s>[INST] {prompt} [/INST]\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def answer(\n",
    "        self, prompt: str, sources: list[Source], *, max_new_tokens: int = 256\n",
    "    ) -> Iterator[str]:\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            self.make_prompt(prompt, sources), add_bos=False\n",
    "        )\n",
    "\n",
    "        self.generator.begin_stream_ex(input_ids, self.settings)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            result = self.generator.stream_ex()\n",
    "            if result[\"eos\"]:\n",
    "                break\n",
    "            yield result[\"chunk\"]\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef967823-bc69-4a06-8e72-9b8713fbaf58",
   "metadata": {},
   "source": [
    "## Use the assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029fa95-b094-4831-857b-5f7fc81430da",
   "metadata": {},
   "source": [
    "You can directly import and start using Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b07a54-12b0-46b8-8bc0-0531978d179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm import Mistral7BInstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a6139-9bb3-4d0d-b338-e9584927b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mistral7BInstruct.display_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24974543-918e-471c-95b7-3af6d4770adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mistral7BInstruct.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f66c85-2b75-4fff-aced-f5cc4da40091",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = Mistral7BInstruct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e3691-1666-4188-97fd-9cc521029b56",
   "metadata": {},
   "source": [
    "Let's share the Python Software Foundation's annual reports, and ask questions about the PSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84700998-f92e-4207-aa68-77bdd0c8f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragna import Rag, source_storages\n",
    "\n",
    "documents = [\n",
    "    \"files/psf-report-2021.pdf\",\n",
    "    \"files/psf-report-2022.pdf\",\n",
    "]\n",
    "\n",
    "chat = Rag().chat(\n",
    "    documents=documents,\n",
    "    source_storage=source_storages.Chroma,\n",
    "    assistant=assistant,\n",
    ")\n",
    "\n",
    "await chat.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719c026-9da5-4d87-9234-7a08bf0171f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = await chat.answer(\"Who is the Python Developer in Residence?\", stream=True)\n",
    "\n",
    "async for chunk in message:\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b0e3b-a38a-4315-b09f-e678becefd0e",
   "metadata": {},
   "source": [
    "We can also verify the sources used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ba02c-fd7f-452e-80a3-52ed00723f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, source in enumerate(message.sources, 1):\n",
    "    print(f\"{idx}. {source.document.name}: {source.location}:\\n\")\n",
    "    print(source.content)\n",
    "    print(\"#\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a2e43-6d6a-46b9-a87d-9b00901d27f9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "_‚ùóÔ∏è **Warning:** Make sure to stop the Jupyter Kernel (in the JupyterLab Menu Bar, click on \"Kernel\" -> \"Interrupt Kernel\") before proceeding._\n",
    "\n",
    "<br>\n",
    "\n",
    "**‚ú® Next: [RAG and LLM Experiments](04-UI-and-experiments.ipynb) ‚Üí**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007efd5a-7df4-4cbe-b7d6-8ab6fb1216cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pycon-de",
   "language": "python",
   "name": "conda-env-global-global-pycon-de-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
