{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c14620e-982a-46bd-b621-6f2cb60d0a13",
   "metadata": {},
   "source": [
    "<img src=\"images/ragna-logo.png\" width=20% align=\"right\"/>\n",
    "\n",
    "# RAG and LLM Experiments\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcd5d8-b9df-490d-a230-f0c4eb520d53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explore the Web UI\n",
    "\n",
    "Ragna ships with a Panel-based chat application, sometimes also referred to as the web UI. You can use this directly, or as an example to build your own applications.\n",
    "\n",
    "To run the Ragna UI from Nebari, open a terminal window and run the following commands.\n",
    "\n",
    "1. Activate the conda environment\n",
    "   \n",
    "```bash\n",
    "conda activate global-pycon-de\n",
    "```\n",
    "\n",
    "2. Start the UI\n",
    "\n",
    "```bash\n",
    "PYTHON_PATH=\"${HOME}:${PYTHON_PATH}\" \\\n",
    "  dotenv --file ~/shared/analyst/.env run -- \\\n",
    "    python -m \\\n",
    "        ragna ui --config ragna.toml\n",
    "```\n",
    "\n",
    "3. Go to https://pycon-tutorial.quansight.dev/user/{USER}/proxy/31477/ (replace {USER} with your Nebari username)\n",
    "\n",
    "### Side note: Local setup instructions 💻\n",
    "\n",
    "On your personal computers, you can directly run: `ragna ui` in your terminal to start the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8ef94-ef70-4f33-b9ee-a4b9384d958f",
   "metadata": {},
   "source": [
    "## Advanced configuration\n",
    "\n",
    "`rag.chat()` takes the following keyword arguments to help you optimize the quality of answers:\n",
    "\n",
    "* `chunk_size` - Size of each chunk (sections of the document that contain context) to use.\n",
    "* `chunk_overlap` - Size of the overlap with previous and next chunk for retrieving additional context for future prompts.\n",
    "* `num_tokens` - Maximum number of context tokens, and in turn the number of document chunks, pulled out of the vector database.\n",
    "\n",
    "You can also set these configurations in the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87167db3-9f9c-43fa-a3a9-cf4801e7519a",
   "metadata": {},
   "source": [
    "## Compare LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6cec3-05c9-4505-a6b4-94870dfa81ea",
   "metadata": {},
   "source": [
    "Orchestration tools like Ragna can be useful for comparing and experimenting with LLMs quickly.\n",
    "\n",
    "In the following cells, let's see how our local LLM, Mistral 7B, compares to OpenAI's GPT 3.5 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2d246-e5f1-49e0-a8b0-acf97094ebd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from local_llm import Mistral7BInstruct\n",
    "from ragna import Rag\n",
    "from ragna.assistants import Gpt4, Gpt35Turbo16k\n",
    "from ragna.source_storages import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b85199-a1cc-4ed6-8f59-5d72198614fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path.home() / Path(\"shared/analyst/.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7ca2f-654b-4121-a8f2-3db482e1b39d",
   "metadata": {},
   "source": [
    "Let's inquire about PSF's annual reports again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5e104-82d3-49d1-ace5-080649f9c058",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"files/psf-report-2022.pdf\",\n",
    "    \"files/psf-report-2021.pdf\",\n",
    "]\n",
    "\n",
    "source_storages = [Chroma]\n",
    "assistants = [Gpt35Turbo16k, Gpt4, Mistral7BInstruct()]\n",
    "\n",
    "prompt = \"What was PSF's net income in 2021 and 2022?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3608b-8647-413b-9542-be50572a583c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = Rag()\n",
    "\n",
    "\n",
    "async def answer_prompt(source_storage, assistant):\n",
    "    async with rag.chat(\n",
    "        documents=documents,\n",
    "        source_storage=source_storage,\n",
    "        assistant=assistant,\n",
    "    ) as chat:\n",
    "        message = await chat.answer(prompt)\n",
    "        return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02054a04-27f3-493a-8f3f-82207adc31ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    (source_storage.display_name(), assistant.display_name()): answer_prompt(\n",
    "        source_storage, assistant\n",
    "    )\n",
    "    for source_storage, assistant in itertools.product(source_storages, assistants)\n",
    "}\n",
    "\n",
    "pprint(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cacba-7c6e-4d88-9f2b-41e7213c0879",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = dict(zip(experiments.keys(), await asyncio.gather(*experiments.values())))\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4570d0-a653-43ec-91a1-7db4645ab661",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "_❗️ **Warning:** Make sure to stop the Jupyter Kernel (in the JupyterLab Menu Bar, click on \"Kernel\" -> \"Interrupt Kernel\") and close this notebook before proceeding._\n",
    "\n",
    "<br>\n",
    "\n",
    "**✨ Next: [Conclusion](05-conclusion.ipynb) →**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b77d4a-709a-48d7-8c71-ce69f45fb27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef530560-bdc1-4372-ae64-89966d14d7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pycon-de",
   "language": "python",
   "name": "conda-env-global-global-pycon-de-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
